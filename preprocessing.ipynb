{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "# List of required packages\n",
    "packages = ['matplotlib', 'plotly', 'pandas', 'numpy', 'datetime', 'seaborn', 'scikit-learn', 'geopandas', 'shapely', 'pyshp']\n",
    "\n",
    "# Check if packages are installed\n",
    "missing_packages = [pkg for pkg in packages if importlib.util.find_spec(pkg) is None]\n",
    "\n",
    "# Install missing packages\n",
    "if missing_packages:\n",
    "    %pip install {' '.join(missing_packages)}\n",
    "\n",
    "# Import the packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import re\n",
    "import geopandas as gpd\n",
    "import shapefile as shp\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clearvars():    \n",
    "    for el in sorted(globals()):\n",
    "        if '__' not in el:\n",
    "                print(f'deleted: {el}')\n",
    "                del el\n",
    "clearvars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'dataRaw/'\n",
    "dataRaw = pd.read_csv(base_path + 'data.csv', sep=',', low_memory=False)\n",
    "holidays_eventsRaw = pd.read_csv(base_path + 'holidays_events(India).csv', sep=';')\n",
    "itemsRaw = pd.read_csv(base_path + 'items.csv', sep=';', low_memory=False)\n",
    "oilRaw = pd.read_csv(base_path + 'oil(India).csv', sep=',', skipinitialspace=True)\n",
    "storesRaw = pd.read_csv(base_path + 'stores.csv', sep=';', encoding='ISO-8859-1')\n",
    "testRaw = pd.read_csv(base_path + 'test.csv', sep=',')\n",
    "transactionsRaw = pd.read_csv(base_path + 'transactions.csv', sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = testRaw.copy()\n",
    "test['date'] = pd.to_datetime(test['date'], format='%Y-%m-%d')\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "\"\"\" test.to_csv('data/test.csv', index=False)\n",
    "> git push origin main:main\n",
    "remote: error: Trace: 25213f19d457fb2989564964b10ab27c2fb682233e594fb81ff2ed78998776c2        \n",
    "remote: error: See https://gh.io/lfs for more information.        \n",
    "remote: error: File data/test.csv is 120.32 MB; this exceeds GitHub's file size limit of 100.00 MB        \n",
    "remote: error: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.        \n",
    "To https://github.com/florboydens/Data-Analytics-Case-2.git\n",
    " ! [remote rejected] main -> main (pre-receive hook declined)\n",
    "error: failed to push some refs to 'https://github.com/florboydens/Data-Analytics-Case-2.git' \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataRaw.copy()\n",
    "data['date'] = pd.to_datetime(dataRaw['date'], format='%Y-%m-%d')\n",
    "data['unit_sales'] = dataRaw['unit_sales'].round(2)\n",
    "data['onpromotion'].fillna(False, inplace=True)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "\"\"\" \n",
    "data.to_csv('data/data.csv', index=False)\n",
    "...\n",
    "Zelfde reden als de Test file, te groot\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = oilRaw['dcoilwtico;;'].str.contains('%')\n",
    "oil = oilRaw[~mask].copy()\n",
    "oil.reset_index(drop=True, inplace=True)\n",
    "oil['dcoilwtico;;'] = oil['dcoilwtico;;'].str.replace(';', '')\n",
    "oil['dcoilwtico;;'] = oil['dcoilwtico;;'].apply(lambda x: x.strip())\n",
    "oil = oil[~(oil['dcoilwtico;;'] == '')]\n",
    "try:\n",
    "    oil['dcoilwtico;;'] = oil['dcoilwtico;;'].astype(float).round(2)\n",
    "except:\n",
    "    oil.dropna(subset=['dcoilwtico;;'], inplace=True)\n",
    "oil.rename(columns={'dcoilwtico;;': 'dcoilwtico'}, inplace=True)\n",
    "\n",
    "oilExtra = pd.read_csv('dataRaw/oil(India).csv', sep=';', names=['date', 'price', 'change', 'extra'], skiprows=1)\n",
    "oilExtra.drop(columns=['extra'], inplace=True)\n",
    "oilExtra.dropna(inplace=True)\n",
    "oilExtra = oilExtra.iloc[:61].copy()\n",
    "oilExtra['price'] = oilExtra['price'].str.replace(',', '').astype(float)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "oil.to_csv('data/oil.csv', index=False)\n",
    "oilExtra.to_csv('data/oil(INR).csv', index=False)\n",
    "\n",
    "del oil, oilExtra\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsRaw = itemsRaw.dropna()\n",
    "try:\n",
    "    itemsRaw['item_nbr'] = itemsRaw['item_nbr'].astype(int)\n",
    "except:\n",
    "    pass\n",
    "itemsRaw['Price'] = itemsRaw['Price'].round(2)\n",
    "itemsRaw['perishable'] = itemsRaw['perishable'].map({0: False, 1: True})\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "itemsRaw.to_csv('data/items.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactionsRaw['date'] = pd.to_datetime(test['date'], format='%Y-%m-%d')\n",
    "transactionsRaw['month_day'] = transactionsRaw['date'].dt.strftime('%m-%d')\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "transactionsRaw.to_csv('data/transactions.csv', index=False)\n",
    "\n",
    "del transactionsRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_eventsRaw = holidays_eventsRaw.drop(columns=['type'])\n",
    "holidays_eventsRaw.dropna(subset=['date'], inplace=True)\n",
    "holidays_eventsRaw['date'] = holidays_eventsRaw['date'].str.replace(' ', '/')\n",
    "holidays_eventsRaw['date'] = holidays_eventsRaw['date'].str.replace('mrt', 'mar')\n",
    "holidays_eventsRaw['date'] = holidays_eventsRaw['date'].str.replace('mei', 'may')\n",
    "holidays_eventsRaw['date'] = holidays_eventsRaw['date'].str.replace('okt', 'oct')\n",
    "holidays_eventsRaw = holidays_eventsRaw.drop_duplicates()\n",
    "holidays_eventsRaw['date'] = pd.to_datetime(holidays_eventsRaw['date'], format='%d/%b')   #.dt.strftime('%Y-%m-%d')\n",
    "# if the date is the same from earlier in the dataset, drop the row.\n",
    "holidays_eventsRaw = holidays_eventsRaw[~holidays_eventsRaw['date'].duplicated()]\n",
    "holidays_eventsRaw['month_day'] = holidays_eventsRaw['date'].dt.strftime('%m-%d')\n",
    "holidays_eventsRaw = holidays_eventsRaw.sort_values('date')\n",
    "holidays_eventsRaw = holidays_eventsRaw.reindex(columns=['date', 'month_day', 'description', 'transferred'])\n",
    "holidays_eventsRaw.rename(columns={'transferred': 'type'}, inplace=True)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "holidays_eventsRaw.to_csv('data/holidays_events.csv', index=False)\n",
    "\n",
    "del holidays_eventsRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all the rows with NaN store_nbr because they are not in the rest of the data set\n",
    "storesRaw.dropna(subset=['store_nbr'], inplace=True)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "storesRaw.to_csv('data/stores.csv', index=False)\n",
    "\n",
    "del storesRaw"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-analytics-kernel-case-2",
   "language": "python",
   "name": "data-analytics-kernel-case-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
