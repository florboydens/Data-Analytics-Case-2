{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\cursist\\myenvforvscode\\lib\\site-packages (1.4.1.post1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: pyshp in c:\\users\\cursist\\myenvforvscode\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\cursist\\myenvforvscode\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\cursist\\myenvforvscode\\lib\\site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\cursist\\myenvforvscode\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\cursist\\myenvforvscode\\lib\\site-packages (from scikit-learn) (3.4.0)\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "# List of required packages\n",
    "packages = ['matplotlib', 'plotly', 'pandas', 'numpy', 'datetime', 'seaborn', 'scikit-learn', 'geopandas', 'shapely', 'pyshp']\n",
    "\n",
    "# Check if packages are installed\n",
    "missing_packages = [pkg for pkg in packages if importlib.util.find_spec(pkg) is None]\n",
    "\n",
    "# Install missing packages\n",
    "if missing_packages:\n",
    "    %pip install {' '.join(missing_packages)}\n",
    "\n",
    "# Import the packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import re\n",
    "import geopandas as gpd\n",
    "import shapefile as shp\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted: In\n",
      "deleted: Out\n",
      "deleted: Point\n",
      "deleted: RandomForestRegressor\n",
      "deleted: _\n",
      "deleted: _13\n",
      "deleted: _14\n",
      "deleted: _22\n",
      "deleted: _23\n",
      "deleted: _31\n",
      "deleted: _32\n",
      "deleted: _40\n",
      "deleted: _41\n",
      "deleted: _5\n",
      "deleted: _6\n",
      "deleted: _9\n",
      "deleted: _dh\n",
      "deleted: _exit_code\n",
      "deleted: _i\n",
      "deleted: _i1\n",
      "deleted: _i10\n",
      "deleted: _i11\n",
      "deleted: _i12\n",
      "deleted: _i13\n",
      "deleted: _i14\n",
      "deleted: _i15\n",
      "deleted: _i16\n",
      "deleted: _i17\n",
      "deleted: _i18\n",
      "deleted: _i19\n",
      "deleted: _i2\n",
      "deleted: _i20\n",
      "deleted: _i21\n",
      "deleted: _i22\n",
      "deleted: _i23\n",
      "deleted: _i24\n",
      "deleted: _i25\n",
      "deleted: _i26\n",
      "deleted: _i27\n",
      "deleted: _i28\n",
      "deleted: _i29\n",
      "deleted: _i3\n",
      "deleted: _i30\n",
      "deleted: _i31\n",
      "deleted: _i32\n",
      "deleted: _i33\n",
      "deleted: _i34\n",
      "deleted: _i35\n",
      "deleted: _i36\n",
      "deleted: _i37\n",
      "deleted: _i38\n",
      "deleted: _i39\n",
      "deleted: _i4\n",
      "deleted: _i40\n",
      "deleted: _i41\n",
      "deleted: _i42\n",
      "deleted: _i43\n",
      "deleted: _i44\n",
      "deleted: _i45\n",
      "deleted: _i46\n",
      "deleted: _i47\n",
      "deleted: _i5\n",
      "deleted: _i6\n",
      "deleted: _i7\n",
      "deleted: _i8\n",
      "deleted: _i9\n",
      "deleted: _ih\n",
      "deleted: _ii\n",
      "deleted: _iii\n",
      "deleted: _oh\n",
      "deleted: actual_file_path\n",
      "deleted: base_path\n",
      "deleted: clearvars\n",
      "deleted: data\n",
      "deleted: dataRaw\n",
      "deleted: datetime\n",
      "deleted: exit\n",
      "deleted: get_ipython\n",
      "deleted: gpd\n",
      "deleted: holidays_events\n",
      "deleted: importlib\n",
      "deleted: items\n",
      "deleted: itemsRaw\n",
      "deleted: mask\n",
      "deleted: matplotlib\n",
      "deleted: mean_squared_error\n",
      "deleted: missing_packages\n",
      "deleted: np\n",
      "deleted: oilExtra_path\n",
      "deleted: oilRaw\n",
      "deleted: open\n",
      "deleted: packages\n",
      "deleted: pd\n",
      "deleted: plotly\n",
      "deleted: plt\n",
      "deleted: px\n",
      "deleted: quit\n",
      "deleted: re\n",
      "deleted: shp\n",
      "deleted: sklearn\n",
      "deleted: sns\n",
      "deleted: stores\n",
      "deleted: test\n",
      "deleted: testRaw\n",
      "deleted: train_test_split\n",
      "deleted: transactions\n"
     ]
    }
   ],
   "source": [
    "def clearvars():    \n",
    "    for el in sorted(globals()):\n",
    "        if '__' not in el:\n",
    "                print(f'deleted: {el}')\n",
    "                del el\n",
    "clearvars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'dataRaw/'\n",
    "dataRaw = pd.read_csv(base_path + 'data.csv', sep=',', low_memory=False)\n",
    "holidays_eventsRaw = pd.read_csv(base_path + 'holidays_events(India).csv', sep=';')\n",
    "itemsRaw = pd.read_csv(base_path + 'items.csv', sep=';', low_memory=False)\n",
    "oilRaw = pd.read_csv(base_path + 'oil(India).csv', sep=',', skipinitialspace=True)\n",
    "storesRaw = pd.read_csv(base_path + 'stores.csv', sep=';', encoding='ISO-8859-1')\n",
    "testRaw = pd.read_csv(base_path + 'test.csv', sep=',')\n",
    "transactionsRaw = pd.read_csv(base_path + 'transactions.csv', sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" test.to_csv('data/test.csv', index=False)\\n> git push origin main:main\\nremote: error: Trace: 25213f19d457fb2989564964b10ab27c2fb682233e594fb81ff2ed78998776c2        \\nremote: error: See https://gh.io/lfs for more information.        \\nremote: error: File data/test.csv is 120.32 MB; this exceeds GitHub's file size limit of 100.00 MB        \\nremote: error: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.        \\nTo https://github.com/florboydens/Data-Analytics-Case-2.git\\n ! [remote rejected] main -> main (pre-receive hook declined)\\nerror: failed to push some refs to 'https://github.com/florboydens/Data-Analytics-Case-2.git' \""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = testRaw.copy()\n",
    "test['date'] = pd.to_datetime(test['date'], format='%Y-%m-%d')\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "\"\"\" test.to_csv('data/test.csv', index=False)\n",
    "> git push origin main:main\n",
    "remote: error: Trace: 25213f19d457fb2989564964b10ab27c2fb682233e594fb81ff2ed78998776c2        \n",
    "remote: error: See https://gh.io/lfs for more information.        \n",
    "remote: error: File data/test.csv is 120.32 MB; this exceeds GitHub's file size limit of 100.00 MB        \n",
    "remote: error: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.        \n",
    "To https://github.com/florboydens/Data-Analytics-Case-2.git\n",
    " ! [remote rejected] main -> main (pre-receive hook declined)\n",
    "error: failed to push some refs to 'https://github.com/florboydens/Data-Analytics-Case-2.git' \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cursist\\AppData\\Local\\Temp\\ipykernel_24960\\705338131.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['onpromotion'].fillna(False, inplace=True)\n",
      "C:\\Users\\Cursist\\AppData\\Local\\Temp\\ipykernel_24960\\705338131.py:4: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['onpromotion'].fillna(False, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" \\ndata.to_csv('data/data.csv', index=False)\\n...\\nZelfde reden als de Test file, te groot\\n\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataRaw.copy()\n",
    "data['date'] = pd.to_datetime(dataRaw['date'], format='%Y-%m-%d')\n",
    "data['unit_sales'] = dataRaw['unit_sales'].round(2)\n",
    "data['onpromotion'].fillna(False, inplace=True)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "\"\"\" \n",
    "data.to_csv('data/data.csv', index=False)\n",
    "...\n",
    "Zelfde reden als de Test file, te groot\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = oilRaw['dcoilwtico;;'].str.contains('%')\n",
    "oil = oilRaw[~mask].copy()\n",
    "oil.reset_index(drop=True, inplace=True)\n",
    "oil['dcoilwtico;;'] = oil['dcoilwtico;;'].str.replace(';', '')\n",
    "oil['dcoilwtico;;'] = oil['dcoilwtico;;'].apply(lambda x: x.strip())\n",
    "oil = oil[~(oil['dcoilwtico;;'] == '')]\n",
    "try:\n",
    "    oil['dcoilwtico;;'] = oil['dcoilwtico;;'].astype(float).round(2)\n",
    "except:\n",
    "    oil.dropna(subset=['dcoilwtico;;'], inplace=True)\n",
    "oil.rename(columns={'dcoilwtico;;': 'dcoilwtico'}, inplace=True)\n",
    "\n",
    "oilExtra = pd.read_csv('dataRaw/oil(India).csv', sep=';', names=['date', 'price', 'change', 'extra'], skiprows=1)\n",
    "oilExtra.drop(columns=['extra'], inplace=True)\n",
    "oilExtra.dropna(inplace=True)\n",
    "oilExtra = oilExtra.iloc[:61].copy()\n",
    "oilExtra['price'] = oilExtra['price'].str.replace(',', '').astype(float)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "oil.to_csv('data/oil.csv', index=False)\n",
    "oilExtra.to_csv('data/oil(INR).csv', index=False)\n",
    "\n",
    "del oil, oilExtra\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsRaw = itemsRaw.dropna()\n",
    "try:\n",
    "    itemsRaw['item_nbr'] = itemsRaw['item_nbr'].astype(int)\n",
    "except:\n",
    "    pass\n",
    "itemsRaw['Price'] = itemsRaw['Price'].round(2)\n",
    "itemsRaw['perishable'] = itemsRaw['perishable'].map({0: False, 1: True})\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "itemsRaw.to_csv('data/items.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactionsRaw['date'] = pd.to_datetime(test['date'], format='%Y-%m-%d')\n",
    "transactionsRaw['month_day'] = transactionsRaw['date'].dt.strftime('%m-%d')\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "transactionsRaw.to_csv('data/transactions.csv', index=False)\n",
    "\n",
    "del transactionsRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_eventsRaw = holidays_eventsRaw.drop(columns=['type'])\n",
    "holidays_eventsRaw.dropna(subset=['date'], inplace=True)\n",
    "holidays_eventsRaw['date'] = holidays_eventsRaw['date'].str.replace(' ', '/')\n",
    "holidays_eventsRaw['date'] = holidays_eventsRaw['date'].str.replace('mrt', 'mar')\n",
    "holidays_eventsRaw['date'] = holidays_eventsRaw['date'].str.replace('mei', 'may')\n",
    "holidays_eventsRaw['date'] = holidays_eventsRaw['date'].str.replace('okt', 'oct')\n",
    "holidays_eventsRaw = holidays_eventsRaw.drop_duplicates()\n",
    "holidays_eventsRaw['date'] = pd.to_datetime(holidays_eventsRaw['date'], format='%d/%b')   #.dt.strftime('%Y-%m-%d')\n",
    "# if the date is the same from earlier in the dataset, drop the row.\n",
    "holidays_eventsRaw = holidays_eventsRaw[~holidays_eventsRaw['date'].duplicated()]\n",
    "holidays_eventsRaw['month_day'] = holidays_eventsRaw['date'].dt.strftime('%m-%d')\n",
    "holidays_eventsRaw = holidays_eventsRaw.sort_values('date')\n",
    "holidays_eventsRaw = holidays_eventsRaw.reindex(columns=['date', 'month_day', 'description', 'transferred'])\n",
    "holidays_eventsRaw.rename(columns={'transferred': 'type'}, inplace=True)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "holidays_eventsRaw.to_csv('data/holidays_events.csv', index=False)\n",
    "\n",
    "del holidays_eventsRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all the rows with NaN store_nbr because they are not in the rest of the data set\n",
    "storesRaw.dropna(subset=['store_nbr'], inplace=True)\n",
    "\n",
    "# --------------------------------------------------------------------------------------------\n",
    "\n",
    "storesRaw.to_csv('data/stores.csv', index=False)\n",
    "\n",
    "del storesRaw"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-analytics-kernel-case-2",
   "language": "python",
   "name": "data-analytics-kernel-case-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
