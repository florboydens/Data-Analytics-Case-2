{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install seaborn\n",
    "# %pip install datetime\n",
    "# %pip install pandas\n",
    "# %pip install scikit-learn\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'data/'\n",
    "holidays_events = pd.read_csv(base_path + 'holidays_events.csv')\n",
    "items = pd.read_csv(base_path + 'items.csv')\n",
    "oil = pd.read_csv(base_path + 'oil.csv')\n",
    "oil_INR = pd.read_csv(base_path + 'oil(INR).csv', sep=',', skipinitialspace=True)\n",
    "stores = pd.read_csv(base_path + 'stores.csv')\n",
    "transactions = pd.read_csv(base_path + 'transactions.csv')\n",
    "\n",
    "test = pd.read_csv('dataRaw/test.csv', sep=',')\n",
    "test['date'] = pd.to_datetime(test['date'], format='%Y-%m-%d')\n",
    "\n",
    "data = pd.read_csv('dataRaw/data.csv', sep=',', low_memory=False)\n",
    "data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d')\n",
    "data['unit_sales'] = data['unit_sales'].round(2)\n",
    "data['onpromotion'].fillna(False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'dataRaw/'\n",
    "dataRaw = pd.read_csv(base_path + 'data.csv', sep=',', low_memory=False)\n",
    "holidays_events = pd.read_csv(base_path + 'holidays_events(India).csv', sep=';')\n",
    "items = pd.read_csv(base_path + 'items.csv', sep=';', low_memory=False)\n",
    "oilRaw = pd.read_csv(base_path + 'oil(India).csv', sep=',', skipinitialspace=True)\n",
    "stores = pd.read_csv(base_path + 'stores.csv', sep=';', encoding='ISO-8859-1')\n",
    "testRaw = pd.read_csv(base_path + 'test.csv', sep=',')\n",
    "transactions = pd.read_csv(base_path + 'transactions.csv', sep=',')\n",
    "\n",
    "mask = oilRaw['dcoilwtico;;'].str.contains('%')\n",
    "oilExtra = oilRaw[mask].copy()\n",
    "oil = oilRaw[~mask].copy()\n",
    "oil.reset_index(drop=True, inplace=True)\n",
    "oil['dcoilwtico;;'] = oil['dcoilwtico;;'].str.replace(';', '')\n",
    "oil['dcoilwtico;;'] = oil['dcoilwtico;;'].apply(lambda x: x.strip())\n",
    "oil = oil[~(oil['dcoilwtico;;'] == '')]\n",
    "\n",
    "try:\n",
    "    oil['dcoilwtico;;'] = oil['dcoilwtico;;'].astype(float).round(2)\n",
    "except:\n",
    "    oil.dropna(subset=['dcoilwtico;;'], inplace=True)\n",
    "\n",
    "oil.rename(columns={'dcoilwtico;;': 'dcoilwtico'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extending the provided sample to include all months up to \"Dec 2017\"\n",
    "date_samples = [\n",
    "    \"jan/13\", \"feb/13\", \"Mar 2013\", \"apr/13\", \"May 2013\", \"jun/13\",\n",
    "    \"jul/13\", \"aug/13\", \"sep/13\", \"Oct 2013\", \"nov/13\", \"dec/13\",\n",
    "    \"jan/14\", \"feb/14\", \"Mar 2014\", \"apr/14\", \"May 2014\", \"jun/14\",\n",
    "    \"jul/14\", \"aug/14\", \"sep/14\", \"Oct 2014\", \"nov/14\", \"dec/14\",\n",
    "    \"jan/15\", \"feb/15\", \"Mar 2015\", \"apr/15\", \"May 2015\", \"jun/15\",\n",
    "    \"jul/15\", \"aug/15\", \"sep/15\", \"Oct 2015\", \"nov/15\", \"dec/15\",\n",
    "    \"jan/16\", \"feb/16\", \"Mar 2016\", \"apr/16\", \"May 2016\", \"jun/16\",\n",
    "    \"jul/16\", \"aug/16\", \"sep/16\", \"Oct 2016\", \"nov/16\", \"dec/16\",\n",
    "    \"jan/17\", \"feb/17\", \"Mar 2017\", \"apr/17\", \"May 2017\", \"jun/17\",\n",
    "    \"jul/17\", \"aug/17\", \"sep/17\", \"Oct 2017\", \"nov/17\", \"dec/17\",\n",
    "    # Including the range \"Jan 2013\" to \"Dec 2017\"\n",
    "    \"Jan 2013\", \"Feb 2013\", \"Mar 2013\", \"Apr 2013\", \"May 2013\", \"Jun 2013\",\n",
    "    \"Jul 2013\", \"Aug 2013\", \"Sep 2013\", \"Oct 2013\", \"Nov 2013\", \"Dec 2013\",\n",
    "    \"Jan 2014\", \"Feb 2014\", \"Mar 2014\", \"Apr 2014\", \"May 2014\", \"Jun 2014\",\n",
    "    \"Jul 2014\", \"Aug 2014\", \"Sep 2014\", \"Oct 2014\", \"Nov 2014\", \"Dec 2014\",\n",
    "    \"Jan 2015\", \"Feb 2015\", \"Mar 2015\", \"Apr 2015\", \"May 2015\", \"Jun 2015\",\n",
    "    \"Jul 2015\", \"Aug 2015\", \"Sep 2015\", \"Oct 2015\", \"Nov 2015\", \"Dec 2015\",\n",
    "    \"Jan 2016\", \"Feb 2016\", \"Mar 2016\", \"Apr 2016\", \"May 2016\", \"Jun 2016\",\n",
    "    \"Jul 2016\", \"Aug 2016\", \"Sep 2016\", \"Oct 2016\", \"Nov 2016\", \"Dec 2016\",\n",
    "    \"Jan 2017\", \"Feb 2017\", \"Mar 2017\", \"Apr 2017\", \"May 2017\", \"Jun 2017\",\n",
    "    \"Jul 2017\", \"Aug 2017\", \"Sep 2017\", \"Oct 2017\", \"Nov 2017\", \"Dec 2017\"\n",
    "]\n",
    "# Define a function to convert date strings to 'YYYY-MM' format using regular expressions\n",
    "def convert_date(date_str):\n",
    "    # Define the regex pattern for matching the date strings\n",
    "    # This pattern matches strings like \"Jan/13\", \"jan/13\", \"Oct 2017\", \"oct 2017\"\n",
    "    pattern = r'(?i)(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)[/ ](\\d{2,4})'\n",
    "    match = re.match(pattern, date_str)\n",
    "    \n",
    "    if match:\n",
    "        month_str, year_str = match.groups()\n",
    "        # Handle two-digit year\n",
    "        if len(year_str) == 2:\n",
    "            year_str = \"20\" + year_str  # Assuming all dates are in the 2000s\n",
    "        \n",
    "        # Convert the month string to a month number, ensuring it's case-insensitive\n",
    "        month_num = datetime.strptime(month_str.title(), '%b').month\n",
    "        # Format the date string as 'YYYY-MM'\n",
    "        return f\"{year_str}-{month_num:02d}\"\n",
    "    else:\n",
    "        # If the date string doesn't match the pattern, return None or handle as needed\n",
    "        return None\n",
    "\n",
    "# Example usage on the sample dates\n",
    "converted_dates_regex = [convert_date(date) for date in date_samples]\n",
    "\n",
    "# Show the first few converted dates for verification\n",
    "converted_dates_regex[:10]\n",
    "\n",
    "\n",
    "# Applying the conversion function to the 'date' column of oilExtra\n",
    "oilExtra['date'] = oilExtra['date'].apply(convert_date)\n",
    "\n",
    "oil_INR['date'] = oil_INR['date'].apply(convert_date)\n",
    "\n",
    "# Applying the conversion function to the 'date' column of holidays_events\n",
    "# holidays_events['date'] = holidays_events['date'].apply(convert_date)\n",
    "\n",
    "oilExtra.head()\n",
    "oil_INR.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vanaf hier gaan we de vragen oplossen en visualizeren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate transactions by store number to find the total sales (transactions) per store\n",
    "total_sales_per_store = transactions.groupby('store_nbr')['transactions'].sum().reset_index()\n",
    "\n",
    "# Find the store with the highest total sales\n",
    "highest_sales_store = total_sales_per_store.loc[total_sales_per_store['transactions'].idxmax()]\n",
    "\n",
    "highest_sales_store\n",
    "\n",
    "# Sort the stores by sales in descending order for better visualization\n",
    "total_sales_per_store_sorted = total_sales_per_store.sort_values('transactions', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.bar(total_sales_per_store_sorted['store_nbr'].astype(str), total_sales_per_store_sorted['transactions'])\n",
    "plt.xlabel('Store Number')\n",
    "plt.ylabel('Total Transactions')\n",
    "plt.title('Total Transactions per Store')\n",
    "plt.xticks(rotation=90)  # Rotate store number labels to avoid overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Highlight the store with the highest sales\n",
    "plt.axhline(y=highest_sales_store['transactions'], color='r', linestyle='--')\n",
    "plt.annotate(f'Highest: Store {highest_sales_store[\"store_nbr\"]}', \n",
    "             xy=(highest_sales_store[\"store_nbr\"] - 1, highest_sales_store['transactions']),\n",
    "             xytext=(highest_sales_store[\"store_nbr\"] - 10, highest_sales_store['transactions'] + 100000),\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "             )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Merge data with items to link each sale to its item family\n",
    "data_items_merged = pd.merge(data, items, on='item_nbr', how='left')\n",
    "\n",
    "# Merge the result with stores to associate each sale with its store's location\n",
    "data_items_stores_merged = pd.merge(data_items_merged, stores, on='store_nbr', how='left')\n",
    "\n",
    "# Filter for sales in West-Bengal\n",
    "west_bengal_sales = data_items_stores_merged[data_items_stores_merged['state'] == 'West Bengal']\n",
    "\n",
    "# Aggregate sales by item family\n",
    "sales_by_family = west_bengal_sales.groupby('family')['unit_sales'].sum().reset_index()\n",
    "\n",
    "# Sort the aggregated sales to find the top-selling item family\n",
    "top_selling_families = sales_by_family.sort_values(by='unit_sales', ascending=False)\n",
    "\n",
    "# The top-selling family can be found at the top of this sorted DataFrame\n",
    "top_selling_family = top_selling_families.iloc[0]\n",
    "\n",
    "top_selling_family \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))  # Sets the figure size\n",
    "plt.bar(top_selling_families['family'], top_selling_families['unit_sales'], color='skyblue')  # Creates a bar plot\n",
    "plt.title('Item Family Sales in West-Bengal')  # Title of the plot\n",
    "plt.xlabel('Item Family')  # Label for the X-axis\n",
    "plt.ylabel('Total Unit Sales')  # Label for the Y-axis\n",
    "plt.xticks(rotation=45, ha='right')  # Rotates the X-axis labels for better readability\n",
    "plt.tight_layout()  # Adjusts subplot params for the plot to fit into the figure area\n",
    "\n",
    "# Highlighting the top-selling item family\n",
    "plt.bar(top_selling_families.iloc[0]['family'], top_selling_families.iloc[0]['unit_sales'], color='gold', label='Top Selling Family')\n",
    "plt.legend()  # Adds a legend to distinguish the top-selling family\n",
    "\n",
    "plt.show()  # Displays the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # 'oil' contains oil prices with 'date' and 'oil_price' columns\n",
    "merged_data = pd.merge(data, oil, on='date', how='left')\n",
    "\n",
    "daily_sales = merged_data.groupby('date')['unit_sales'].sum().reset_index()\n",
    "\n",
    "daily_data_combined = pd.merge(daily_sales, oil, on='date', how='left')\n",
    "\n",
    "plt.scatter(daily_data_combined['oil_price'], daily_data_combined['unit_sales'])\n",
    "plt.xlabel('Oil Price')\n",
    "plt.ylabel('Total Daily Sales')\n",
    "plt.title('Relationship Between Oil Price and Total Daily Sales')\n",
    "plt.show()\n",
    "\n",
    "correlation_coefficient = daily_data_combined['oil_price'].corr(daily_data_combined['unit_sales'])\n",
    "print(f'Pearson correlation coefficient: {correlation_coefficient}') \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" plt.figure(figsize=(10, 6))\n",
    "plt.scatter(daily_data_combined['oil_price'], daily_data_combined['unit_sales'], alpha=0.5)\n",
    "plt.title('Oil Price vs. Total Daily Sales')\n",
    "plt.xlabel('Oil Price')\n",
    "plt.ylabel('Total Daily Sales')\n",
    "plt.grid(True)\n",
    "plt.show()plt.figure(figsize=(10, 6))\n",
    "plt.scatter(daily_data_combined['oil_price'], daily_data_combined['unit_sales'], alpha=0.5)\n",
    "plt.title('Oil Price vs. Total Daily Sales')\n",
    "plt.xlabel('Oil Price')\n",
    "plt.ylabel('Total Daily Sales')\n",
    "plt.grid(True)\n",
    "plt.show() \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting 'date' column to datetime format and extracting day of the week\n",
    "#transactions['date'] = pd.to_datetime(transactions['date'])\n",
    "transactions['day_of_week'] = transactions['date'].dt.day_name()\n",
    "\n",
    "# Grouping the data by day of the week to calculate the average transactions\n",
    "average_transactions_by_day = transactions.groupby('day_of_week')['transactions'].mean().reset_index()\n",
    "\n",
    "# Ordering the days of the week for plotting\n",
    "day_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "average_transactions_by_day['day_of_week'] = pd.Categorical(average_transactions_by_day['day_of_week'], categories=day_order, ordered=True)\n",
    "average_transactions_by_day = average_transactions_by_day.sort_values('day_of_week')\n",
    "\n",
    "# Plotting the average transactions by day of the week\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='day_of_week', y='transactions', data=average_transactions_by_day, palette='coolwarm')\n",
    "plt.title('Average Transactions by Day of the Week')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Average Transactions')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(holidays.head())\n",
    "print(transactions.head())\n",
    "\n",
    "# add a column 'is_holiday' to transactions: if the month_day in transactions has a occurance in holidays, set this column to true otherwise set it to false.\n",
    "transactions['is_holiday'] = transactions['month_day'].isin(holidays['month_day'])\n",
    "\n",
    "transactions_on_date = transactions[transactions['month_day'] == '01-01']\n",
    "if transactions_on_date.empty:\n",
    "    print(\"No transaction on this new year date\")\n",
    "else:\n",
    "    print(\"There is a transaction on this new year date\")\n",
    "\n",
    "# ...there are no transactions on the holidays, lets check data.\n",
    "\"\"\" dataRaw = pd.read_csv('dataRaw/data.csv', sep=',', low_memory=False)\n",
    "data = dataRaw.copy()\n",
    "data['date'] = pd.to_datetime(dataRaw['date'], format='%Y-%m-%d')\n",
    "data['unit_sales'] = dataRaw['unit_sales'].round(2)\n",
    "data['onpromotion'].fillna(False, inplace=True)\n",
    "print(data.head()) \"\"\"\n",
    "# data['month_day'] = pd.to_datetime(data['date']).dt.strftime('%m-%d')\n",
    "print(data.head())\n",
    "\n",
    "data_on_date = data[data['month_day'] == '01-01']\n",
    "if data_on_date.empty:\n",
    "    print(\"No data on this new year date\")\n",
    "else:\n",
    "    print(\"There is data on this new year date\")\n",
    "\n",
    "# Count unit sales on holidays\n",
    "unit_sales_holidays = data[data['month_day'].isin(holidays['month_day'])]['unit_sales'].sum()\n",
    "\n",
    "# Count unit sales on non-holidays\n",
    "unit_sales_no_holidays = data[~data['month_day'].isin(holidays['month_day'])]['unit_sales'].sum()\n",
    "\n",
    "# Plot the results in a bar chart\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = ['Holidays', 'No Holidays']\n",
    "values = [unit_sales_holidays, unit_sales_no_holidays]\n",
    "\n",
    "plt.bar(labels, values)\n",
    "plt.xlabel('Date Type')\n",
    "plt.ylabel('Unit Sales')\n",
    "plt.title('Unit Sales on Holidays vs No Holidays')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter data for holidays\n",
    "data_holidays = data[data['month_day'].isin(holidays['month_day'])]\n",
    "\n",
    "# Filter data for no holidays\n",
    "data_no_holidays = data[~data['month_day'].isin(holidays['month_day'])]\n",
    "\n",
    "# Calculate average unit sales for holidays and no holidays\n",
    "average_unit_sales_holidays = data_holidays['unit_sales'].mean()\n",
    "average_unit_sales_no_holidays = data_no_holidays['unit_sales'].mean()\n",
    "average_unit_sales = data['unit_sales'].mean()\n",
    "\n",
    "# Plot the results in a bar chart\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = ['Holidays', 'No Holidays']\n",
    "values = [average_unit_sales_holidays, average_unit_sales_no_holidays]\n",
    "\n",
    "plt.bar(labels, values)\n",
    "plt.xlabel('Date Type')\n",
    "plt.ylabel('Average Unit Sales')\n",
    "plt.title('Average Unit Sales on Holidays vs No Holidays')\n",
    "plt.show()\n",
    "# Filter data for holidays\n",
    "data_holidays = data[data['month_day'].isin(holidays['month_day'])]\n",
    "\n",
    "# Filter data for no holidays\n",
    "data_no_holidays = data[~data['month_day'].isin(holidays['month_day'])]\n",
    "\n",
    "# Calculate average unit sales for holidays and no holidays\n",
    "average_unit_sales_holidays = data_holidays['unit_sales'].mean()\n",
    "average_unit_sales_no_holidays = data_no_holidays['unit_sales'].mean()\n",
    "\n",
    "# Compare the average unit sales\n",
    "if average_unit_sales_holidays > average_unit_sales_no_holidays:\n",
    "    print(\"Average unit sales are higher on holidays.\")\n",
    "elif average_unit_sales_holidays < average_unit_sales_no_holidays:\n",
    "    print(\"Average unit sales are higher on normal days.\")\n",
    "else:\n",
    "    print(\"Average unit sales are the same on holidays and no holidays.\")\n",
    "print(\"Average Unit Sales:\", average_unit_sales)\n",
    "print(\"Average Unit Sales on Holidays:\", average_unit_sales_holidays)\n",
    "print(\"Average Unit Sales on No Holidays:\", average_unit_sales_no_holidays)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Merge the transactions with the holiday data\n",
    "transactions_with_holidays = pd.merge(\n",
    "    transactions,\n",
    "    holidays_events,\n",
    "    on='date',\n",
    "    how='left'\n",
    ") \"\"\"\n",
    "\n",
    "# Mark transactions that happened on holidays\n",
    "transactions_with_holidays['is_holiday'] = ~transactions_with_holidays['transferred'].isnull()\n",
    "\n",
    "# Visualize the distribution of transactions on holidays vs non-holidays using boxplots\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# First plot for all data\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(x='is_holiday', y='transactions', data=transactions_with_holidays)\n",
    "plt.title('Transaction Distribution with Outliers')\n",
    "plt.xlabel('Is Holiday')\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.xticks([0, 1], ['Non-Holiday', 'Holiday'])\n",
    "\n",
    "# Second plot without outliers\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x='is_holiday', y='transactions', data=transactions_with_holidays, showfliers=False)\n",
    "plt.title('Transaction Distribution without Outliers')\n",
    "plt.xlabel('Is Holiday')\n",
    "plt.xticks([0, 1], ['Non-Holiday', 'Holiday'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate the average transactions on holidays vs. non-holidays\n",
    "avg_trans_holiday_comparison = transactions_with_holidays.groupby('is_holiday')['transactions'].mean().reset_index()\n",
    "\n",
    "# Print the result\n",
    "print(\"Average transactions on non-holidays:\", avg_trans_holiday_comparison.loc[avg_trans_holiday_comparison['is_holiday'] == False, 'transactions'].values[0])\n",
    "print(\"Average transactions on holidays:\", avg_trans_holiday_comparison.loc[avg_trans_holiday_comparison['is_holiday'] == True, 'transactions'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.info())\n",
    "# Calculate average unit sales by day of the week\n",
    "average_sales_by_day = data.groupby('date').mean()['unit_sales']\n",
    "\n",
    "# Subset of data where dates are in the holiday dataframe\n",
    "holiday_sales_by_day = data[data['date'].isin(holidays_events['date'])]#.groupby('date').mean()['unit_sales']\n",
    "print(holiday_sales_by_day)\n",
    "\n",
    "# Create a bar chart\n",
    "plt.bar(average_sales_by_day.index, average_sales_by_day, label='Average Unit Sales')\n",
    "plt.bar(holiday_sales_by_day.index, holiday_sales_by_day, label='Holiday Unit Sales')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Average Unit Sales')\n",
    "plt.title('Average Unit Sales by Day of the Week')\n",
    "\n",
    "# Show the legend\n",
    "plt.legend()\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Check if the DataFrame is not empty\n",
    "if not top_selling_families.empty:\n",
    "    plt.figure(figsize=(12, 8))  # Sets the figure size\n",
    "    plt.bar(top_selling_families['family'], top_selling_families['unit_sales'], color='skyblue')  # Creates a bar plot\n",
    "    plt.title('Item Family Sales in West-Bengal')  # Title of the plot\n",
    "    plt.xlabel('Item Family')  # Label for the X-axis\n",
    "    plt.ylabel('Total Unit Sales')  # Label for the Y-axis\n",
    "    plt.xticks(rotation=45, ha='right')  # Rotates the X-axis labels for better readability\n",
    "    plt.tight_layout()  # Adjusts subplot params so the plot fits into the figure area\n",
    "    \n",
    "    # Highlight the top-selling item family\n",
    "    plt.bar(top_selling_families.iloc[0]['family'], top_selling_families.iloc[0]['unit_sales'], color='gold', label='Top Selling Family')\n",
    "    plt.legend()  # Adds a legend to distinguish the top-selling family\n",
    "    \n",
    "    plt.show()  # Displays the plot\n",
    "else:\n",
    "    print(\"The quest yields no data! Check if 'West-Bengal' is correctly specified or if the data contains relevant entries.\") \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick overview of each DataFrame\n",
    "for df in [data, holidays_events, items, oil, oilExtra, stores, transactions]:\n",
    "    display(df.head())\n",
    "    display(df.info())\n",
    "    display(df.describe())\n",
    "\n",
    "# Quick overview of each DataFrame\n",
    "data_overview = {\n",
    "    \"DataFrame\": [\"data\", \"holidays_events\", \"items\", \"oil\", \"oilExtra\", \"stores\", \"transactions\"],\n",
    "    \"First_Five_Rows\": [df.head() for df in [data, holidays_events, items, oil, oilExtra, stores, transactions]],\n",
    "    \"Info\": [df.info() for df in [data, holidays_events, items, oil, oilExtra, stores, transactions]],\n",
    "    \"Describe\": [df.describe() for df in [data, holidays_events, items, oil, oilExtra, stores, transactions]]\n",
    "}\n",
    "\n",
    "data_overview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = pd.to_datetime(data['date'])\n",
    "data['year'] = data['date'].dt.year\n",
    "data['month'] = data['date'].dt.month\n",
    "data['day'] = data['date'].dt.day\n",
    "data['day_of_week'] = data['date'].dt.dayofweek  # Monday=0, Sunday=6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating Sales Data on a Weekly or Monthly Basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = pd.to_datetime(data['date'])\n",
    "data['year'] = data['date'].dt.year\n",
    "data['month'] = data['date'].dt.month\n",
    "data['day'] = data['date'].dt.day\n",
    "data['day_of_week'] = data['date'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "\n",
    "holidays_events['date'] = pd.to_datetime(holidays_events['date'], errors='coerce', infer_datetime_format=True)\n",
    "data['is_holiday'] = data['date'].isin(holidays_events['date']).astype(int)\n",
    "\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a 'month_year' feature for easier aggregation by month\n",
    "data['month_year'] = data['date'].dt.to_period('M')\n",
    "\n",
    "# Aggregating sales data monthly\n",
    "monthly_sales = data.groupby(['store_nbr', 'item_nbr', 'month_year'])['unit_sales'].sum().reset_index()\n",
    "monthly_sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Binary Variables for Holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marking holidays in the main dataset\n",
    "data['is_holiday'] = data['date'].isin(holidays_events['date']).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sales Data Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating sales data monthly, using 'year' and 'month' for grouping\n",
    "monthly_sales_simplified = data.groupby(['store_nbr', 'item_nbr', 'year', 'month'])['unit_sales'].sum().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['is_holiday'] = data['date'].isin(holidays_events['date']).astype(int)\n",
    "\n",
    "# Now, let's proceed with the simplified monthly aggregation of sales data.\n",
    "# We're grouping by 'store_nbr', 'item_nbr', 'year', and 'month', then summing up the 'unit_sales' to get monthly sales.\n",
    "# This approach aggregates sales data at a monthly level without relying on the 'month_year' period column, \n",
    "# which avoids the error encountered previously.\n",
    "monthly_sales_simplified = data.groupby(['store_nbr', 'item_nbr', 'year', 'month'])['unit_sales'].sum().reset_index()\n",
    "\n",
    "# Display the head of the newly created binary variable column and the simplified monthly sales to verify our steps.\n",
    "display(data[['date', 'is_holiday']].head())\n",
    "monthly_sales_simplified.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0   id       date  store_nbr  item_nbr  unit_sales  onpromotion  \\\n",
      "0           0    0 2013-01-01         25    103665         7.0        False   \n",
      "1          60   60 2013-01-01         25    214381         5.0        False   \n",
      "2         120  120 2013-01-01         25    315179         4.0        False   \n",
      "3         180  180 2013-01-01         25    414752         1.0        False   \n",
      "4         240  240 2013-01-01         25    557408         3.0        False   \n",
      "\n",
      "     price change  \n",
      "0  5708.32  3.29%  \n",
      "1  5708.32  3.29%  \n",
      "2  5708.32  3.29%  \n",
      "3  5708.32  3.29%  \n",
      "4  5708.32  3.29%  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '3.29%'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(merged_data\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Calculate the correlation matrix\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m correlation_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mmerged_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Extract the correlation of interest\u001b[39;00m\n\u001b[0;32m     17\u001b[0m oil_sales_correlation \u001b[38;5;241m=\u001b[39m correlation_matrix\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munit_sales\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Replace 'price' as needed\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Cursist\\myenvforvscode\\Lib\\site-packages\\pandas\\core\\frame.py:11036\u001b[0m, in \u001b[0;36mDataFrame.corr\u001b[1;34m(self, method, min_periods, numeric_only)\u001b[0m\n\u001b[0;32m  11034\u001b[0m cols \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m  11035\u001b[0m idx \u001b[38;5;241m=\u001b[39m cols\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m> 11036\u001b[0m mat \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m  11038\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpearson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m  11039\u001b[0m     correl \u001b[38;5;241m=\u001b[39m libalgos\u001b[38;5;241m.\u001b[39mnancorr(mat, minp\u001b[38;5;241m=\u001b[39mmin_periods)\n",
      "File \u001b[1;32mc:\\Users\\Cursist\\myenvforvscode\\Lib\\site-packages\\pandas\\core\\frame.py:1981\u001b[0m, in \u001b[0;36mDataFrame.to_numpy\u001b[1;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[0;32m   1979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1980\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(dtype)\n\u001b[1;32m-> 1981\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dtype:\n\u001b[0;32m   1983\u001b[0m     result \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(result, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Cursist\\myenvforvscode\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1692\u001b[0m, in \u001b[0;36mBlockManager.as_array\u001b[1;34m(self, dtype, copy, na_value)\u001b[0m\n\u001b[0;32m   1690\u001b[0m         arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1691\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1692\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interleave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1693\u001b[0m     \u001b[38;5;66;03m# The underlying data was copied within _interleave, so no need\u001b[39;00m\n\u001b[0;32m   1694\u001b[0m     \u001b[38;5;66;03m# to further copy if copy=True or setting na_value\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_value \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n",
      "File \u001b[1;32mc:\\Users\\Cursist\\myenvforvscode\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1751\u001b[0m, in \u001b[0;36mBlockManager._interleave\u001b[1;34m(self, dtype, na_value)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1750\u001b[0m         arr \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mget_values(dtype)\n\u001b[1;32m-> 1751\u001b[0m     \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m arr\n\u001b[0;32m   1752\u001b[0m     itemmask[rl\u001b[38;5;241m.\u001b[39mindexer] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m itemmask\u001b[38;5;241m.\u001b[39mall():\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '3.29%'"
     ]
    }
   ],
   "source": [
    "\n",
    "oil_INR.rename(columns={'oil_date': 'date'}, inplace=True)  # Replace 'oil_date' as needed\n",
    "\n",
    "# Convert 'date' columns to datetime format\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "oil_INR['date'] = pd.to_datetime(oil_INR['date'])\n",
    "\n",
    "# Merge the datasets on the 'date' column\n",
    "merged_data = pd.merge(data, oil_INR, on='date', how='inner')\n",
    "\n",
    "# Check the first few rows to ensure it merged correctly\n",
    "print(merged_data.head())\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = merged_data.corr()\n",
    "\n",
    "# Extract the correlation of interest\n",
    "oil_sales_correlation = correlation_matrix.loc['unit_sales', 'price']  # Replace 'price' as needed\n",
    "\n",
    "print(f\"The correlation coefficient between oil prices and unit sales is: {oil_sales_correlation}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-analytics-kernel-case-2",
   "language": "python",
   "name": "data-analytics-kernel-case-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
